{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ayman Abdullah \n",
    "#3700039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "program\n",
      "program\n",
      "program\n",
      "cake\n",
      "indic\n",
      "matric\n"
     ]
    }
   ],
   "source": [
    "#Exercise 2: Assume you have a list of words that you want to get their stem, write python script that find stem ofeach word in the list.\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter = PorterStemmer() \n",
    "\n",
    "list=['dogs', 'programming', 'programs', 'programmed',\n",
    "      'cakes', 'indices', 'matrices']\n",
    "for word in list:\n",
    "    print(Stemmerporter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs            -> dog\n",
      "programming     -> program\n",
      "programs        -> program\n",
      "programmed      -> program\n",
      "cakes           -> cake\n",
      "indices         -> indic\n",
      "matrices        -> matric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "porter = PorterStemmer()\n",
    "l_words = ['dogs', 'programming', 'programs', 'programmed', 'cakes', 'indices', 'matrices']\n",
    "for word in l_words:\n",
    "    print(f'{word} \\t -> {porter.stem(word)}'.expandtabs(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A               -> A\n",
      "stemmer         -> stemmer\n",
      "for             -> for\n",
      "English         -> english\n",
      "operating       -> oper\n",
      "on              -> on\n",
      "the             -> the\n",
      "stem            -> stem\n",
      "cat             -> cat\n",
      "sh              -> sh\n",
      "ould            -> ould\n",
      "identify        -> identifi\n",
      "such            -> such\n",
      "strings         -> string\n",
      "as              -> as\n",
      "cats            -> cat\n",
      ",               -> ,\n",
      "catlike         -> catlik\n",
      ",               -> ,\n",
      "and             -> and\n",
      "catty           -> catti\n",
      ".               -> .\n",
      "A               -> A\n",
      "stem            -> stem\n",
      "ming            -> ming\n",
      "algorithm       -> algorithm\n",
      "might           -> might\n",
      "also            -> also\n",
      "reduce          -> reduc\n",
      "the             -> the\n",
      "words           -> word\n",
      "fishing         -> fish\n",
      ",               -> ,\n",
      "fished          -> fish\n",
      ",               -> ,\n",
      "an              -> an\n",
      "d               -> d\n",
      "fisher          -> fisher\n",
      "to              -> to\n",
      "the             -> the\n",
      "stem            -> stem\n",
      "fish            -> fish\n",
      ".               -> .\n",
      "The             -> the\n",
      "stem            -> stem\n",
      "need            -> need\n",
      "not             -> not\n",
      "be              -> be\n",
      "a               -> a\n",
      "word            -> word\n",
      ",               -> ,\n",
      "for             -> for\n",
      "ex              -> ex\n",
      "ample           -> ampl\n",
      "the             -> the\n",
      "Porter          -> porter\n",
      "algorithm       -> algorithm\n",
      "reduces         -> reduc\n",
      ",               -> ,\n",
      "argue           -> argu\n",
      ",               -> ,\n",
      "argued          -> argu\n",
      ",               -> ,\n",
      "argues          -> argu\n",
      ",               -> ,\n",
      "arg             -> arg\n",
      "uing            -> u\n",
      ",               -> ,\n",
      "and             -> and\n",
      "argus           -> argu\n",
      "to              -> to\n",
      "the             -> the\n",
      "stem            -> stem\n",
      "argu            -> argu\n",
      ".               -> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "sentence = '''A stemmer for English operating on the stem cat sh\n",
    "ould identify such strings as cats, catlike, and catty. A stem\n",
    "ming algorithm might also reduce the words fishing, fished, an\n",
    "d fisher to the stem fish. The stem need not be a word, for ex\n",
    "ample the Porter algorithm reduces, argue, argued, argues, arg\n",
    "uing, and argus to the stem argu.'''\n",
    "\n",
    "# tokenize the sentence\n",
    "list = nltk.word_tokenize(sentence)\n",
    "\n",
    "# print each word in the sentence before and after Stemming \n",
    "for word in list:\n",
    "    print(f'{word} \\t -> {porter.stem(word)}'.expandtabs(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A stemmer for english oper on the stem cat should identifi such string as cat , catlik , and catti . A stem algorithm might also reduc the word fish , fish , and fisher to the stem fish'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence = \"A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish\"\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "tokenized_sentence = []\n",
    "for word in tokenized_words:\n",
    "    tokenized_sentence.append(porter.stem(word))\n",
    "tokenized_sentence = \" \".join(tokenized_sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3: Repeat the task presented in exercise 2. Then,compare outputs! Do you find a difference between Porter\n",
    "#Stemmer outputs and Lancaster Stemmer? Why???\n",
    "#The cutting method differs, and this indicates that the methods differ for the roller\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 4: Check how to perform stemming using SnowballStemmer and Write your python script below! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حرك\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "w='حركات'\n",
    "print(st.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "علم البيانات علم يغلب عليه تداخل الاختصاصات والذي يقوم على استخدام الأساليب العلمية، والمعالجات والخوارزميات والنظم لاستخراج المعرفة والأفكار من البيانات بشكليها، سواء مُهيكلة، أو غير مهيكلة، بشكل مشابه للتنقيب في البيانات. كما يعتمد علم البيانات على تقنيات تعلم الآلة والذكاء الصناعي وبرامج معالجة البيانات الضخمة\n",
      "Stemmed sentence\n",
      "علم بين علم غلب عليه دخل خصص والذي يقم على خدم الب علمية، علج خوارزم نظم لاستخراج عرف فكر من بين بشكليها، سوء مهيكلة، او غير مهيكلة، شكل شبه نقب في بين . كما عمد علم بين على تقن علم الة ذكء صنع رمج علج بين ضخم \n"
     ]
    }
   ],
   "source": [
    "file=open(r'C:\\Users\\DELL\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Anaconda3 (64-bit)\\DS.txt',encoding = 'UTF-8')\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "st = ISRIStemmer()\n",
    "\n",
    "Sentences= file.read()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words: \n",
    "        stem_sentence.append(st.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "    \n",
    "print(Sentences)\n",
    "print(\"Stemmed sentence\")\n",
    "x = stemSentence(Sentences)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    " if word in punctuations:\n",
    "    \n",
    "    sentence_words.remove(word)\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "#Exercise 5. Now compare the output? What did you see? Why? \n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations: sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
